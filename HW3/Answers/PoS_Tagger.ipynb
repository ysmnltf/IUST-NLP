{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V742_8AWtokB"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rxEc_HuUUYA4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import treebank\n",
        "import pprint\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PChSoL1tokH"
      },
      "source": [
        "# Datasets download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O4WyhH9WL2f",
        "outputId": "37c6f5be-468a-4562-926c-32e9f994d0ac",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('treebank')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4oSLnW1tokJ"
      },
      "source": [
        "# Tagged sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GYqC2mfUj_S",
        "outputId": "09f6ed66-64d5-4f64-9769-55d909e8284c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18\n"
          ]
        }
      ],
      "source": [
        "tagged_sentences = treebank.tagged_sents()\n",
        "print(len(tagged_sentences[0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_word_feature(word, index):\n",
        "  # create features from the word and its position\n",
        "  return {  \n",
        "      'word': word, \n",
        "      'is_first': index == 0, \n",
        "      'is_capitalized': word[0].upper() == word[0],     \n",
        "      'is_all_caps': word.upper() == word,     \n",
        "      'is_all_lower': word.lower() == word,        \n",
        "      'prefix-1': word[0],       \n",
        "      'prefix-2': word[:2],      \n",
        "      'prefix-3': word[:3],       \n",
        "      'suffix-1': word[-1],     \n",
        "      'suffix-2': word[-2:],      \n",
        "      'suffix-3': word[-3:],      \n",
        "      'has_hyphen': '-' in word,  \n",
        "      'is_numeric': word.isdigit(),   \n",
        "      'capitals_inside': word[1:].lower() != word[1:],\n",
        "      'is_short': len(word) < 4,\n",
        "      'is_long': len(word) > 10,\n",
        "  }\n",
        "\n",
        "# create dataset\n",
        "X, y = [], []\n",
        "\n",
        "for tagged_sent in tagged_sentences:\n",
        "    untagged = [w for w,t in tagged_sent]\n",
        "    for index in range(len(untagged)):\n",
        "        X.append(get_word_feature(untagged[index], index))\n",
        "        y.append(tagged_sent[index][1])\n",
        "\n",
        "# split data into 80% train 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
      ],
      "metadata": {
        "id": "2AGW5xZEdtGl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create classifier pipeline\n",
        "classifier=Pipeline([\n",
        "                     ('vectorizer', DictVectorizer(sparse=False)),\n",
        "                     ('classifier', DecisionTreeClassifier(criterion='entropy')\n",
        "                    )])\n",
        "# train\n",
        "classifier.fit(X_train[:50000], y_train[:50000])\n",
        "\n",
        "# get accuracy\n",
        "print(\"acc: \", classifier.score(X_test, y_test))\n",
        "\n",
        "# predict tags\n",
        "def pos_tag(tokens): \n",
        "  features = [get_word_feature(tokens[index], index, len(tokens)) for index in range(len(tokens))]\n",
        "  tags = classifier.predict(features)\n",
        "  return zip(tokens, tags)\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnJJ_uz1V9yi",
        "outputId": "6eebbd63-f770-4928-d5e3-dd9b1a01b771"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  0.9241160111243544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrvplCp7tokK"
      },
      "source": [
        "# Sample for output of your PoS tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlBIyChncJIZ",
        "outputId": "31105c03-d3d4-4103-ada0-1cae176c9648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('my', 'NN'), ('friend', 'NN'), (',', ','), ('John', 'NNP'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "print(list(pos_tag(word_tokenize('This is my friend, John.'))))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "PoS_Tagger.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
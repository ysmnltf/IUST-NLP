{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34kSvaXZejxP"
      },
      "source": [
        "# **HW2 - Text Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAnj55oxdpiQ"
      },
      "source": [
        "## 1. IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElC24F9DdRxq",
        "outputId": "79fc8b1c-e45e-4d69-f9d6-717e3b1c2472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "Example of dataset: \n",
            "Text:  <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all\n",
            "Sentiment:  True\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import imdb\n",
        "INDEX_FROM = 3\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(index_from=INDEX_FROM, num_words=5000)\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "# Reverse the word index to obtain a dict mapping indices to words\n",
        "word_index = {k:(v+INDEX_FROM) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n",
        "\n",
        "# Decode the first sequence in the dataset\n",
        "print(\"Example of dataset: \")\n",
        "print(\"Text: \", \" \".join(inverted_word_index[i] for i in x_train[0]))\n",
        "print(\"Sentiment: \", y_train.astype(bool)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpXc193Vd30r"
      },
      "source": [
        "## 2. Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tXfDtZQhINU"
      },
      "source": [
        "### 2.1. Any data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9czIJs0lFKv",
        "outputId": "ca2931d9-0ef2-4bb5-8791-d01249bc8ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jL5KqGCQhHZy"
      },
      "outputs": [],
      "source": [
        "train_text = []\n",
        "\n",
        "# remove punctuation\n",
        "for x in x_train:\n",
        "  text = \" \".join(inverted_word_index[i] for i in x[1:])\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  train_text.append(text)\n",
        "\n",
        "test_text = []\n",
        "\n",
        "# remove punctuation\n",
        "for x in x_test:\n",
        "  text = \" \".join(inverted_word_index[i] for i in x[1:])\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  test_text.append(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwIwqPNulRVn",
        "outputId": "1c9f13e4-ba9c-4c94-ef06-0321f94252cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# other cleaning is done after tokenization\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "english_stopwords = list(set(stopwords.words('english')))\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer=SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3YfvFW8g1ee"
      },
      "source": [
        "### 2.2. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Td99_RE5do2S"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "train_tokens = []\n",
        "for text in train_text:\n",
        "  tokens = word_tokenize(text)\n",
        "  # remove stop words\n",
        "  stemmed_tokens=[(stemmer.stem(i)).lower() for i in tokens]\n",
        "  filtered_tokens = [token for token in stemmed_tokens if token not in english_stopwords]\n",
        "  train_tokens.append(filtered_tokens)\n",
        "\n",
        "\n",
        "test_tokens = []\n",
        "for text in test_text:\n",
        "  tokens = word_tokenize(text)\n",
        "  # remove stop words\n",
        "  stemmed_tokens=[(stemmer.stem(i)).lower() for i in tokens]\n",
        "  filtered_tokens = [token for token in stemmed_tokens if token not in english_stopwords]\n",
        "  test_tokens.append(filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-31y4Flg6Z4"
      },
      "source": [
        "### 2.3. Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVF1_XMLg88W"
      },
      "outputs": [],
      "source": [
        "# done in previus part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imQqe33JeZqS"
      },
      "source": [
        "## 3. Build Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opNqhDhai4oJ"
      },
      "source": [
        "### 3.1. Uni-Gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2LltG5vQjGJv"
      },
      "outputs": [],
      "source": [
        "class classifier:\n",
        "  def __init__(self, train_tokens, y, classes_n=2):\n",
        "    self.classes_n = classes_n\n",
        "    self.train_tokens = train_tokens\n",
        "    self.y = y\n",
        "    # self.p_ci[0] = neg\n",
        "    # self.p_ci[1] = pos\n",
        "    \n",
        "    # creating vocab\n",
        "    self.vocab, self.vocab_pos, self.vocab_neg = self.build_vocab()\n",
        "    self.vocab[\"unk\"]=0\n",
        "    self.vocab_pos[\"unk\"]=0\n",
        "    self.vocab_neg[\"unk\"]=0\n",
        "\n",
        "    # number of words in each class\n",
        "    self.ns = np.zeros((2,1))\n",
        "    self.ns[1] = sum(self.vocab_pos.values())\n",
        "    self.ns[0] = sum(self.vocab_neg.values())\n",
        "\n",
        "    # index to word and word to index arrays\n",
        "    self.itow = list(self.vocab.keys())\n",
        "    self.wtoi = {w:idx for idx,w in enumerate(self.itow)}\n",
        "\n",
        "    # calculating probabilities\n",
        "    self.p_ci = np.zeros((classes_n,1))\n",
        "    self.p_wk_cj = np.zeros((len(self.vocab), classes_n))\n",
        "\n",
        "    self.find_p_ci()\n",
        "    self.find_p_wk_cj()\n",
        "\n",
        "\n",
        "  def build_vocab(self):\n",
        "    vocab = dict()\n",
        "    vocab_pos = dict()\n",
        "    vocab_neg = dict()\n",
        "\n",
        "    for i in range(len(self.train_tokens)):\n",
        "      tokens = self.train_tokens[i]\n",
        "      for token in tokens:\n",
        "        # add token to general vocab\n",
        "        n = vocab.get(token, 0)\n",
        "        vocab[token] = n + 1\n",
        "        # add token to its class\n",
        "        if (self.y[i] == 0):\n",
        "          n = vocab_neg.get(token, 0)\n",
        "          vocab_neg[token] = n + 1\n",
        "        else:\n",
        "          n = vocab_pos.get(token, 0)\n",
        "          vocab_pos[token] = n + 1\n",
        "\n",
        "    return vocab, vocab_pos, vocab_neg\n",
        "\n",
        "  def find_p_ci(self):\n",
        "    pos_n = np.count_nonzero(self.y)\n",
        "    neg_n = len(self.y) - pos_n\n",
        "    self.p_ci[1] = pos_n / len(self.y)\n",
        "    self.p_ci[0] = 1 - self.p_ci[1]\n",
        "\n",
        "  def find_p_wk_cj(self):\n",
        "    words = self.itow\n",
        "\n",
        "    # negative, j = 0\n",
        "    for k in range(len(words)):\n",
        "      w = words[k]\n",
        "      self.p_wk_cj[k, 0] = ((self.vocab_neg.get(w, 0)+1) / (self.ns[0] + len(self.itow)+1))\n",
        "\n",
        "    # positive, j = 1\n",
        "    for k in range(len(words)):\n",
        "      w = words[k]\n",
        "      self.p_wk_cj[k, 1] = ((self.vocab_pos.get(w, 0)+1) / (self.ns[1] + len(self.itow)+1))\n",
        "\n",
        "\n",
        "  def predict(self, test_tokens):\n",
        "    ps = []\n",
        "    pred = []\n",
        "    for test in test_tokens:\n",
        "      p = np.ones((self.classes_n, 1))\n",
        "      # add probability of class\n",
        "      p[0] = self.p_ci[0]\n",
        "      p[1] = self.p_ci[1]\n",
        "      for token in test:\n",
        "        for c in range(self.classes_n):\n",
        "          if token in self.wtoi:\n",
        "            token_idx = self.wtoi[token]\n",
        "            # add (instead of multiply because of log) probabiliy of word belonging to class c\n",
        "            p[c] += (np.log(self.p_wk_cj[token_idx, c]))\n",
        "          else:\n",
        "            # add 1/dominator if token doesnt exist in vocab (no occurence in train set)\n",
        "            p[c] += (np.log(1/(self.ns[c] + len(self.itow)+1)))\n",
        "            \n",
        "      ps.append(p)\n",
        "      pred.append(np.argmax(p))\n",
        "    return np.array(ps), np.array(pred)\n",
        "\n",
        "\n",
        "# use classifier on tokens (unigrams)\n",
        "unigram_classifier = classifier(train_tokens, y_train)\n",
        "uni_ps, uni_pred = unigram_classifier.predict(test_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I1UwalSjA63"
      },
      "source": [
        "### 3.2. Bi-Gram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "svqLOB5pVXWt"
      },
      "outputs": [],
      "source": [
        "# create bigrams\n",
        "train_bigrams = []\n",
        "for tokens in train_tokens:\n",
        "  bigrams = []\n",
        "  for i in range(len(tokens)-1):\n",
        "    bigrams.append(f\"{tokens[i]} {tokens[i+1]}\")\n",
        "  train_bigrams.append(bigrams)\n",
        "\n",
        "test_bigrams = []\n",
        "for tokens in test_tokens:\n",
        "  bigrams = []\n",
        "  for i in range(len(tokens)-1):\n",
        "    bigrams.append(f\"{tokens[i]} {tokens[i+1]}\")\n",
        "  test_bigrams.append(bigrams)\n",
        "\n",
        "# use classifier on bigrams\n",
        "bigram_classifier = classifier(train_bigrams, y_train)\n",
        "bi_ps, bi_pred = bigram_classifier.predict(test_bigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCICu7qpjI9z"
      },
      "source": [
        "### 3.3. Tri-Gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "t70q9YLTjIWD"
      },
      "outputs": [],
      "source": [
        "# create trigrams\n",
        "train_trigrams = []\n",
        "for tokens in train_tokens:\n",
        "  trigrams = []\n",
        "  for i in range(len(tokens)-2):\n",
        "    trigrams.append(f\"{tokens[i]} {tokens[i+1]} {tokens[i+2]}\")\n",
        "  train_trigrams.append(trigrams)\n",
        "\n",
        "test_trigrams = []\n",
        "for tokens in test_tokens:\n",
        "  trigrams = []\n",
        "  for i in range(len(tokens)-2):\n",
        "    trigrams.append(f\"{tokens[i]} {tokens[i+1]} {tokens[i+2]}\")\n",
        "  test_trigrams.append(trigrams)\n",
        "\n",
        "# use classifier on trigrams\n",
        "trigram_classifier = classifier(train_trigrams, y_train)\n",
        "tri_ps, tri_pred = trigram_classifier.predict(test_trigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJj1mXJJd7eR"
      },
      "source": [
        "## 4. Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcfxW01ceY_R",
        "outputId": "3fa68ea8-10c0-4cbf-b171-75e3f983c356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model      recall    precision    f1score    accuracy\n",
            "-------  --------  -----------  ---------  ----------\n",
            "unigram   0.81504     0.851555   0.832897     0.83648\n",
            "bigram    0.8516      0.866363   0.858918     0.86012\n",
            "trigram   0.79064     0.812479   0.801411     0.80408\n"
          ]
        }
      ],
      "source": [
        "def evaluate(truth, pred):\n",
        "    # true positive, truth = 1 and pred = 1\n",
        "    TP = np.sum(np.logical_and(pred == 1, truth == 1))\n",
        "\n",
        "    # condition positive\n",
        "    P = np.sum(truth)\n",
        "\n",
        "    recall = TP/P\n",
        "\n",
        "    # false positive, truth = 0 but pred = 0\n",
        "    FP = np.sum(np.logical_and(pred == 1, truth == 0))\n",
        "\n",
        "    precision = TP / (TP + FP)\n",
        "\n",
        "    f1score = 2*((precision*recall)/(precision+recall))\n",
        "    \n",
        "    # true negative, truth = 0 and pred = 0\n",
        "    TN = np.sum(np.logical_and(pred == 0, truth == 0))\n",
        "\n",
        "    \n",
        "    accuracy = (TP + TN) / len(truth)\n",
        "\n",
        "    return recall, precision, f1score, accuracy\n",
        "\n",
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "recall1, precision1, f1score1, accuracy1 = evaluate(y_test, np.array(uni_pred))\n",
        "recall2, precision2, f1score2, accuracy2 = evaluate(y_test, np.array(bi_pred))\n",
        "recall3, precision3, f1score3, accuracy3 = evaluate(y_test, np.array(tri_pred))\n",
        "print(tabulate([[\"unigram\", recall1, precision1, f1score1, accuracy1],\n",
        "                [\"bigram\", recall2, precision2, f1score2, accuracy2],\n",
        "                [\"trigram\", recall3, precision3, f1score3, accuracy3]], \n",
        "               headers=[\"model\", \"recall\", \"precision\", \"f1score\", \"accuracy\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7TzueYpKYEb",
        "outputId": "4154c6e1-4185-40bf-e4e1-ca7ecb4da635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------\n",
            "unigram incorrect prediction | sentiment:  1 ; text: \n",
            "im absolut unk movi isnt sold love movi unk disney unk demand theyd eventu sell id buy copi everybodi know everyth everybodi movi good job havent figur whi disney hasnt put movi dvd vhs rental unk least havent seen ani copi wick good movi seen kid new generat dont get see think least put back channel movi doesnt deserv cheap unk deserv real thing im movi dvd\n",
            "token job, pos count: 1565, neg count: 894\n",
            "token theyd, pos count: 49, neg count: 84\n",
            "token put, pos count: 1529, neg count: 1599\n",
            "token hasnt, pos count: 208, neg count: 163\n",
            "token love, pos count: 6142, neg count: 2817\n",
            "token get, pos count: 6459, neg count: 7638\n",
            "token havent, pos count: 436, neg count: 363\n",
            "token cheap, pos count: 201, neg count: 689\n",
            "token isnt, pos count: 1328, neg count: 1847\n",
            "token think, pos count: 4290, neg count: 4619\n",
            "token buy, pos count: 476, neg count: 467\n",
            "token absolut, pos count: 797, neg count: 1040\n",
            "token back, pos count: 2540, neg count: 2419\n",
            "token whi, pos count: 1845, neg count: 3456\n",
            "token figur, pos count: 547, neg count: 585\n",
            "token everyth, pos count: 1121, neg count: 1189\n",
            "token sold, pos count: 83, neg count: 79\n",
            "token see, pos count: 7480, neg count: 6615\n",
            "token good, pos count: 7744, neg count: 7459\n",
            "token wick, pos count: 68, neg count: 48\n",
            "token everybodi, pos count: 203, neg count: 175\n",
            "token dont, pos count: 3300, neg count: 5145\n",
            "token new, pos count: 2450, neg count: 1849\n",
            "token thing, pos count: 3490, neg count: 4686\n",
            "token disney, pos count: 456, neg count: 296\n",
            "token eventu, pos count: 393, neg count: 327\n",
            "token doesnt, pos count: 1932, neg count: 2600\n",
            "token channel, pos count: 187, neg count: 330\n",
            "token kid, pos count: 1323, neg count: 1698\n",
            "token im, pos count: 1906, neg count: 2921\n",
            "token vhs, pos count: 206, neg count: 77\n",
            "token deserv, pos count: 611, neg count: 558\n",
            "token copi, pos count: 361, neg count: 303\n",
            "token seen, pos count: 3413, neg count: 3264\n",
            "token generat, pos count: 186, neg count: 128\n",
            "token demand, pos count: 122, neg count: 81\n",
            "token id, pos count: 593, neg count: 753\n",
            "token movi, pos count: 22636, neg count: 28999\n",
            "token know, pos count: 3608, neg count: 3894\n",
            "token least, pos count: 1102, neg count: 2011\n",
            "token dvd, pos count: 1289, neg count: 996\n",
            "token rental, pos count: 69, neg count: 145\n",
            "token ani, pos count: 3002, neg count: 4657\n",
            "token real, pos count: 2556, neg count: 2160\n",
            "token sell, pos count: 169, neg count: 191\n",
            "-----------------------------------------------------\n",
            "bigram incorrect prediction | sentiment:  1 ; text: \n",
            "im absolut unk movi isnt sold love movi unk disney unk demand theyd eventu sell id buy copi everybodi know everyth everybodi movi good job havent figur whi disney hasnt put movi dvd vhs rental unk least havent seen ani copi wick good movi seen kid new generat dont get see think least put back channel movi doesnt deserv cheap unk deserv real thing im movi dvd\n",
            "token love movi, pos count: 348, neg count: 84\n",
            "token everyth everybodi, pos count: 3, neg count: 2\n",
            "token buy copi, pos count: 11, neg count: 7\n",
            "token figur whi, pos count: 20, neg count: 28\n",
            "token movi seen, pos count: 84, neg count: 77\n",
            "token unk deserv, pos count: 97, neg count: 73\n",
            "token vhs rental, pos count: 2, neg count: 1\n",
            "token disney unk, pos count: 52, neg count: 34\n",
            "token movi dvd, pos count: 28, neg count: 20\n",
            "token cheap unk, pos count: 44, neg count: 125\n",
            "token im absolut, pos count: 3, neg count: 1\n",
            "token put back, pos count: 11, neg count: 14\n",
            "token unk least, pos count: 135, neg count: 270\n",
            "token see think, pos count: 9, neg count: 4\n",
            "token unk movi, pos count: 2171, neg count: 2892\n",
            "token real thing, pos count: 34, neg count: 19\n",
            "token im movi, pos count: 3, neg count: 6\n",
            "token get see, pos count: 120, neg count: 110\n",
            "token know everyth, pos count: 16, neg count: 12\n",
            "token movi doesnt, pos count: 114, neg count: 138\n",
            "token movi good, pos count: 170, neg count: 149\n",
            "token movi isnt, pos count: 80, neg count: 99\n",
            "token havent seen, pos count: 217, neg count: 138\n",
            "token movi unk, pos count: 2174, neg count: 2852\n",
            "token put movi, pos count: 14, neg count: 27\n",
            "token everybodi know, pos count: 7, neg count: 3\n",
            "token everybodi movi, pos count: 1, neg count: 5\n",
            "token rental unk, pos count: 17, neg count: 28\n",
            "token channel movi, pos count: 3, neg count: 14\n",
            "token whi disney, pos count: 3, neg count: 1\n",
            "token absolut unk, pos count: 126, neg count: 128\n",
            "token think least, pos count: 5, neg count: 12\n",
            "token seen ani, pos count: 24, neg count: 41\n",
            "token good job, pos count: 224, neg count: 127\n",
            "token new generat, pos count: 18, neg count: 6\n",
            "token doesnt deserv, pos count: 5, neg count: 32\n",
            "token dvd vhs, pos count: 20, neg count: 1\n",
            "token ani copi, pos count: 1, neg count: 3\n",
            "token thing im, pos count: 6, neg count: 8\n",
            "token dont get, pos count: 143, neg count: 273\n",
            "token unk disney, pos count: 90, neg count: 60\n",
            "-----------------------------------------------------\n",
            "trigram incorrect prediction | sentiment:  1 ; text: \n",
            "im absolut unk movi isnt sold love movi unk disney unk demand theyd eventu sell id buy copi everybodi know everyth everybodi movi good job havent figur whi disney hasnt put movi dvd vhs rental unk least havent seen ani copi wick good movi seen kid new generat dont get see think least put back channel movi doesnt deserv cheap unk deserv real thing im movi dvd\n",
            "token unk disney unk, pos count: 18, neg count: 10\n",
            "token movi good job, pos count: 3, neg count: 1\n",
            "token love movi unk, pos count: 17, neg count: 6\n",
            "token unk movi isnt, pos count: 8, neg count: 12\n",
            "token absolut unk movi, pos count: 2, neg count: 8\n",
            "token dont get see, pos count: 10, neg count: 4\n"
          ]
        }
      ],
      "source": [
        "uni_incorrect_pred = np.arange(len(test_tokens))[uni_pred != y_test]\n",
        "bi_incorrect_pred = np.arange(len(test_tokens))[uni_pred != y_test]\n",
        "tri_incorrect_pred = np.arange(len(test_tokens))[uni_pred != y_test]\n",
        "\n",
        "def compare_tokens(tokens, classifier):\n",
        "  tokens = set(tokens)\n",
        "  for token in tokens:\n",
        "    if (token not in classifier.vocab_pos or token not in classifier.vocab_neg):\n",
        "      continue\n",
        "    if (classifier.vocab_pos[token] == classifier.vocab_neg[token]):\n",
        "      continue\n",
        "    print(f\"token {token}, pos count: {classifier.vocab_pos[token]}, neg count: {classifier.vocab_neg[token]}\")\n",
        "\n",
        "print(\"-----------------------------------------------------\")\n",
        "print(\"unigram incorrect prediction | sentiment: \", y_test[uni_incorrect_pred[0]], \"; text: \")\n",
        "print(\" \".join(test_tokens[uni_incorrect_pred[0]]))\n",
        "compare_tokens(test_tokens[uni_incorrect_pred[0]], unigram_classifier)\n",
        "print(\"-----------------------------------------------------\")\n",
        "print(\"bigram incorrect prediction | sentiment: \", y_test[bi_incorrect_pred[0]], \"; text: \")\n",
        "print(\" \".join(test_tokens[bi_incorrect_pred[0]]))\n",
        "compare_tokens(test_bigrams[bi_incorrect_pred[0]], bigram_classifier)\n",
        "print(\"-----------------------------------------------------\")\n",
        "print(\"trigram incorrect prediction | sentiment: \", y_test[tri_incorrect_pred[0]], \"; text: \")\n",
        "print(\" \".join(test_tokens[tri_incorrect_pred[0]]))\n",
        "compare_tokens(test_trigrams[tri_incorrect_pred[0]], trigram_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_eu0sLUzvR7"
      },
      "source": [
        "## Good Luck!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_HW2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
